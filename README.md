# CardioRisk+ â€” Plateforme End-to-End de PrÃ©diction du Risque Cardiovasculaire

**Module :** Python for Data Science 2  
**Encadrant :** Haythem Ghazouani  
**DurÃ©e :** 7 semaines  
**AnnÃ©e universitaire :** 2025â€“2026  

---

##  Membres de lâ€™Ã©quipe

Ce projet est rÃ©alisÃ© par une Ã©quipe de quatre Ã©tudiants dans le cadre du module *Python for Data Science 2*.

- **Nour Ben Hassine** â€” Acquisition des donnÃ©es, web scraping & analyse exploratoire (EDA)  
- **Nouha Ben Khelil** â€” PrÃ©traitement des donnÃ©es & feature engineering  
- **Hadir Felli** â€” ModÃ©lisation Machine Learning & suivi des expÃ©riences avec MLflow  
- **Nouha Briki** â€” Backend FastAPI, frontend React & dÃ©ploiement Docker  

---

## 1. PrÃ©sentation du projet

**CardioRisk+** est un projet **Machine Learning de bout en bout (end-to-end)** dÃ©veloppÃ© dans le cadre du module *Python for Data Science 2*.  
Il suit un **cycle de vie professionnel du Machine Learning**, allant de **lâ€™acquisition des donnÃ©es brutes et du web scraping** jusquâ€™au **dÃ©ploiement dâ€™une application conteneurisÃ©e prÃªte pour la production**.

Lâ€™objectif principal du projet est de **prÃ©dire le risque de maladies cardiovasculaires** Ã  partir de donnÃ©es cliniques et comportementales, enrichies par des **facteurs de risque mÃ©dicaux collectÃ©s via web scraping Ã  partir de sources de santÃ© publiques**.

Ce projet respecte strictement la **roadmap officielle du module** ainsi que les **critÃ¨res dâ€™Ã©valuation dÃ©finis par lâ€™enseignant**.

## ğŸ—ï¸ Architecture globale du projet

Le schÃ©ma ci-dessous prÃ©sente lâ€™architecture complÃ¨te du projet **CardioRisk+**, depuis
lâ€™acquisition des donnÃ©es et le web scraping jusquâ€™au dÃ©ploiement de lâ€™application.

![Architecture du projet CardioRisk+](assets/d6d53bd4-da19-4eb9-a552-5f1e46e04885.png)

---

## 2. ProblÃ©matique et objectifs

### 2.1 ProblÃ©matique

Les maladies cardiovasculaires constituent lâ€™une des principales causes de mortalitÃ© dans le monde.  
Une dÃ©tection prÃ©coce du risque, basÃ©e sur des indicateurs cliniques et des habitudes de vie, permettrait dâ€™amÃ©liorer considÃ©rablement les stratÃ©gies de prÃ©vention et de suivi mÃ©dical.

### 2.2 Objectifs du projet

Concevoir et dÃ©ployer un **pipeline Machine Learning reproductible, scalable et interprÃ©table**, capable de :

- analyser les facteurs de risque cardiovasculaire,
- prÃ©dire le niveau de risque de dÃ©velopper une maladie cardiovasculaire,
- exposer les prÃ©dictions via une **API REST**,
- proposer une **interface utilisateur intuitive** pour la visualisation des rÃ©sultats.

---

## 3. TÃ¢che de Machine Learning

- **Type de ML :** Classification supervisÃ©e  
- **Variable cible :** Risque de maladie cardiovasculaire  
  - Classification binaire : *Faible risque / Risque Ã©levÃ©*  
  - *(Extension possible vers une classification multi-classes : faible / moyen / Ã©levÃ©)*

---

## 4. Sources de donnÃ©es

### 4.1 Datasets principaux

Le projet sâ€™appuie sur des **datasets de santÃ© publics et validÃ©s**, contenant des variables cliniques et comportementales telles que :

- Ã¢ge, sexe,
- pression artÃ©rielle,
- taux de cholestÃ©rol,
- indice de masse corporelle (IMC),
- diabÃ¨te,
- tabagisme,
- activitÃ© physique.

Ces datasets proviennent de **rÃ©fÃ©rentiels open data reconnus** (ex. UCI, CDC).

### 4.2 Sources de Web Scraping (publiques et Ã©thiques)

Afin dâ€™enrichir les donnÃ©es, des informations mÃ©dicales structurÃ©es sont collectÃ©es par **web scraping** Ã  partir de sites de santÃ© publics, notamment :

- World Health Organization (WHO),
- Centers for Disease Control and Prevention (CDC),
- Mayo Clinic,
- NHS,
- MedlinePlus.

Les donnÃ©es scrapÃ©es incluent :
- facteurs de risque cardiovasculaire,
- recommandations de prÃ©vention,
- descriptions de symptÃ´mes.

> Aucune donnÃ©e personnelle ou sensible nâ€™est collectÃ©e.  
> Toutes les sources utilisÃ©es sont publiques et exploitÃ©es exclusivement Ã  des fins pÃ©dagogiques.

---

## 5. Architecture et workflow du projet

Le projet suit un **workflow Machine Learning complet** :

1. Web scraping et acquisition des donnÃ©es  
2. Nettoyage et prÃ©traitement des donnÃ©es  
3. Analyse exploratoire des donnÃ©es (EDA)  
4. Feature engineering  
5. Gestion du dÃ©sÃ©quilibre des classes (SMOTE)  
6. EntraÃ®nement et Ã©valuation des modÃ¨les  
7. Suivi des expÃ©riences avec MLflow  
8. DÃ©veloppement du backend avec FastAPI  
9. DÃ©veloppement du frontend avec React  
10. Conteneurisation avec Docker et docker-compose  

---

## 6. Roadmap officielle (7 semaines)

- **Semaine 1 :** Configuration de lâ€™environnement, web scraping & EDA  
- **Semaine 2 :** PrÃ©traitement des donnÃ©es & gestion du dÃ©sÃ©quilibre (SMOTE)  
- **Semaine 3 :** ModÃ©lisation avancÃ©e & tracking des expÃ©riences avec MLflow  
- **Semaine 4 :** DÃ©veloppement du backend (FastAPI : health checks & batch prediction)  
- **Semaine 5 :** DÃ©veloppement du frontend avec React  
- **Semaine 6 :** Conteneurisation avec Docker & docker-compose  
- **Semaine 7 :** Revue finale du projet & intÃ©gration CI/CD (optionnelle)  

---

## 7. Technologies utilisÃ©es

### Data Science & Machine Learning
- Python  
- Pandas, NumPy  
- scikit-learn  
- imbalanced-learn (SMOTE)  
- XGBoost / Random Forest  
- MLflow  

### Backend
- FastAPI  
- Pydantic  
- Uvicorn  

### Frontend
- React  
- Vite  
- Axios  

### DevOps & DÃ©ploiement
- Docker  
- docker-compose  
- Git & GitHub  

---

## 8. Alignement avec les critÃ¨res dâ€™Ã©valuation

Ce projet satisfait lâ€™ensemble des critÃ¨res dâ€™Ã©valuation officiels :

- **Data Pipeline (20 %)**  
  QualitÃ© du web scraping et pertinence de lâ€™EDA  

- **Excellence ML (30 %)**  
  Utilisation correcte des pipelines, de SMOTE, de GridSearchCV et de MLflow  

- **API & Interface utilisateur (30 %)**  
  Robustesse des endpoints FastAPI et qualitÃ© de lâ€™interface React  

- **DÃ©ploiement (20 %)**  
  ExÃ©cution complÃ¨te et fonctionnelle via Docker  

---

## 9. Structure du dÃ©pÃ´t
â”œâ”€â”€ code/ # Scripts Python modulaires (data, ML, API)
â”œâ”€â”€ data/ # Ã‰chantillons de donnÃ©es et modÃ¨les sÃ©rialisÃ©s
â”œâ”€â”€ frontend/ # Code source du frontend React
â”œâ”€â”€ tutos/ # Tutoriels fournis par lâ€™enseignant (PDF)
â”œâ”€â”€ cours/ # Supports de cours
â”œâ”€â”€ docs/ # Cahier des charges et documentation
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile.backend
â”œâ”€â”€ Dockerfile.frontend
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## 10. Organisation de lâ€™Ã©quipe

Le projet est dÃ©veloppÃ© de maniÃ¨re collaborative par les quatre membres de lâ€™Ã©quipe.  
Les contributions sont suivies via **GitHub (commits, branches et pull requests)** afin dâ€™assurer la traÃ§abilitÃ© et lâ€™Ã©quitÃ© du travail.

---

## 11. ConsidÃ©rations Ã©thiques et acadÃ©miques

- Aucune donnÃ©e de santÃ© personnelle ou identifiable nâ€™est utilisÃ©e.
- Toutes les sources de donnÃ©es sont publiques et open access.
- Le projet est strictement acadÃ©mique et non commercial.

---

## 12. Licence

Ce projet est distribuÃ© sous licence **MIT** dans un cadre pÃ©dagogique.
